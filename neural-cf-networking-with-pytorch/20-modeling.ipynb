{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map_dict = utils.open_object(\"./artifacts/user_id_map_dict.pkl\")\n",
    "movie_id_map_dict = utils.open_object(\"./artifacts/movie_id_map_dict.pkl\")\n",
    "genres_map_dict = utils.open_object(\"./artifacts/genres_map_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = len(user_id_map_dict)\n",
    "num_movie = len(movie_id_map_dict)\n",
    "num_genere = len(genres_map_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict['num_user'] = num_user \n",
    "config_dict['num_item'] = num_movie \n",
    "config_dict['num_genere'] = num_genere\n",
    "config_dict['latent_dim_mlp'] =  128\n",
    "config_dict['latent_dim_mf']=config_dict['latent_dim_mlp']\n",
    "config_dict['layers'] = [config_dict['latent_dim_mf']*2]+[64,32]\n",
    "config_dict['dropout_rate_mf']=0.2\n",
    "config_dict['dropout_rate_mlp']=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(dictionary=config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch\n",
    "\n",
    "class NeuMF(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # matrix factorization part\n",
    "        self.embedding_user_mf = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_user, embedding_dim=self.config.latent_dim_mf)\n",
    "        self.embedding_item_mf = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_item, embedding_dim=self.config.latent_dim_mf)\n",
    "\n",
    "        # multilayer perceptron part\n",
    "        self.embedding_user_mlp = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_user, embedding_dim=self.config.latent_dim_mlp)\n",
    "        self.embedding_item_mlp = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_item, embedding_dim=self.config.latent_dim_mlp)\n",
    "\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.config.layers[:-1], self.config.layers[1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.logits = torch.nn.Linear(\n",
    "            in_features=self.config.layers[-1] + self.config.latent_dim_mf, out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        # mf part: element-wise product\n",
    "        mf_vector = torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "        mf_vector = torch.nn.Dropout(self.config.dropout_rate_mf)(mf_vector)\n",
    "\n",
    "        # mlp part\n",
    "        # the concat latent vector\n",
    "        mlp_vector = torch.cat(\n",
    "            [user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "            \"\"\"\n",
    "            1) The sigmoid function restricts each\n",
    "            neuron to be in (0,1), which may limit the model's perfor-\n",
    "            mance; and it is known to suffer from saturation, where\n",
    "            neurons stop learning when their output is near either 0 or\n",
    "            1. 2) Even though tanh is a better choice and has been\n",
    "            widely adopted [6, 44], it only alleviates the issues of sig-\n",
    "            moid to a certain extent, since it can be seen as a rescaled\n",
    "            version of sigmoid. And 3) as\n",
    "            such, we opt for ReLU, which is more biologically plausi-\n",
    "            ble and proven to be non-saturated [9]; moreover, it encour-\n",
    "            ages sparse activations, being well-suited for sparse data and\n",
    "            making the model less likely to be overfitting. Our empirical\n",
    "            results show that ReLU yields slightly better performance\n",
    "            than tanh, which in turn is significantly better than sigmoid.\n",
    "            \"\"\"\n",
    "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
    "\n",
    "        mlp_vector = torch.nn.Dropout(self.config.dropout_rate_mlp)(mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.logits(vector)\n",
    "        output = self.sigmoid(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommender(user_indices=sample_dataset['user_embed_id'],\n",
    "#             item_indices=sample_dataset['movie_embed_id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_parquet(\"./data/processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_genres = max([ len(x) for x in df_processed['genres_embed_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data, max_genres=10):\n",
    "        self.data = data\n",
    "        self.max_genres = max_genres\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_item = self.data.iloc[index]\n",
    "        \n",
    "        user_embed_id = data_item[\"user_embed_id\"]\n",
    "        movie_embed_id = data_item[\"movie_embed_id\"]\n",
    "        # genres_embed_ids = data_item[\"genres_embed_ids\"]\n",
    "        # genres_embed_ids = [torch.tensor(ids) for ids in genres_embed_ids]\n",
    "        # padded_genres_embed_ids = pad_sequence(\n",
    "        #     genres_embed_ids, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # padded_genres_embed_ids = padded_genres_embed_ids[:, :self.max_genres]\n",
    "\n",
    "        rating = self.data.iloc[index][\"rating\"]\n",
    "\n",
    "        sample = {\n",
    "            \"user_embed_id\": torch.tensor(user_embed_id, dtype=torch.long),\n",
    "            \"movie_embed_id\": torch.tensor(movie_embed_id, dtype=torch.long),\n",
    "            # \"genres_embed_ids\": padded_genres_embed_ids,\n",
    "            \"rating\": torch.tensor(rating, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import RatingDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./data/processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df,test_size=0.1,random_state=33,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(\"./data/train.parquet\")\n",
    "df_test.to_parquet(\"./data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RatingDataset(data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = NeuMF(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender(user_indices=sample['user_embed_id'],\n",
    "            item_indices=sample['movie_embed_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
