{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map_dict = utils.open_object(\"./artifacts/user_id_map_dict.pkl\")\n",
    "movie_id_map_dict = utils.open_object(\"./artifacts/movie_id_map_dict.pkl\")\n",
    "genres_map_dict = utils.open_object(\"./artifacts/genres_map_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = len(user_id_map_dict)\n",
    "num_movie = len(movie_id_map_dict)\n",
    "num_genre = len(genres_map_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict={}\n",
    "config_dict['num_user'] = num_user \n",
    "config_dict['num_item'] = num_movie \n",
    "config_dict['num_genre'] = num_genre\n",
    "config_dict['latent_dim_mlp'] =  128\n",
    "config_dict['latent_dim_mf']=config_dict['latent_dim_mlp']\n",
    "config_dict['layers'] = [config_dict['latent_dim_mf']*2]+[64,32]\n",
    "config_dict['num_layers'] = len(config_dict['layers'])\n",
    "config_dict['dropout_rate_mf']=0.6\n",
    "config_dict['dropout_rate_mlp']=0.6\n",
    "config_dict['batch_size']=32\n",
    "config_dict['epoches']=8\n",
    "config_dict['eval_steps']=500\n",
    "config_dict['use_xavier_uniform']=False\n",
    "config_dict['learning_rate']=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(dictionary=config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch\n",
    "\n",
    "class NeuMF(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # matrix factorization part\n",
    "        self.embedding_user_mf = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_user, embedding_dim=self.config.latent_dim_mf)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding_user_mf.weight)\n",
    "\n",
    "        self.embedding_item_mf = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_item, embedding_dim=self.config.latent_dim_mf)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding_item_mf.weight)\n",
    "\n",
    "        # multilayer perceptron part\n",
    "        self.embedding_user_mlp = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_user, embedding_dim=self.config.latent_dim_mlp)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding_user_mlp.weight)\n",
    "\n",
    "        self.embedding_item_mlp = torch.nn.Embedding(\n",
    "            num_embeddings=self.config.num_item, embedding_dim=self.config.latent_dim_mlp)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding_item_mlp.weight)\n",
    "\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.config.layers[:-1], self.config.layers[1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.logits = torch.nn.Linear(\n",
    "            in_features=self.config.layers[-1] + self.config.latent_dim_mf, out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        # mf part: element-wise product\n",
    "        mf_vector = torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "        mf_vector = torch.nn.Dropout(self.config.dropout_rate_mf)(mf_vector)\n",
    "\n",
    "        # mlp part\n",
    "        # the concat latent vector\n",
    "        mlp_vector = torch.cat(\n",
    "            [user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "            \"\"\"\n",
    "            1) The sigmoid function restricts each\n",
    "            neuron to be in (0,1), which may limit the model's perfor-\n",
    "            mance; and it is known to suffer from saturation, where\n",
    "            neurons stop learning when their output is near either 0 or\n",
    "            1. 2) Even though tanh is a better choice and has been\n",
    "            widely adopted [6, 44], it only alleviates the issues of sig-\n",
    "            moid to a certain extent, since it can be seen as a rescaled\n",
    "            version of sigmoid. And 3) as\n",
    "            such, we opt for ReLU, which is more biologically plausi-\n",
    "            ble and proven to be non-saturated [9]; moreover, it encour-\n",
    "            ages sparse activations, being well-suited for sparse data and\n",
    "            making the model less likely to be overfitting. Our empirical\n",
    "            results show that ReLU yields slightly better performance\n",
    "            than tanh, which in turn is significantly better than sigmoid.\n",
    "            \"\"\"\n",
    "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
    "\n",
    "        mlp_vector = torch.nn.Dropout(self.config.dropout_rate_mlp)(mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.logits(vector)\n",
    "        output = self.sigmoid(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NeuMF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_parquet(\"./data/processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_genres = max([ len(x) for x in df_processed['genres_embed_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data, max_genres=10):\n",
    "        self.data = data\n",
    "        self.max_genres = max_genres\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_item = self.data.iloc[index]\n",
    "        \n",
    "        user_embed_id = data_item[\"user_embed_id\"]\n",
    "        movie_embed_id = data_item[\"movie_embed_id\"]\n",
    "        # genres_embed_ids = data_item[\"genres_embed_ids\"]\n",
    "        # genres_embed_ids = [torch.tensor(ids) for ids in genres_embed_ids]\n",
    "        # padded_genres_embed_ids = pad_sequence(\n",
    "        #     genres_embed_ids, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # padded_genres_embed_ids = padded_genres_embed_ids[:, :self.max_genres]\n",
    "\n",
    "        rating = self.data.iloc[index][\"rating\"]\n",
    "\n",
    "        sample = {\n",
    "            \"user_embed_id\": torch.tensor(user_embed_id, dtype=torch.long),\n",
    "            \"movie_embed_id\": torch.tensor(movie_embed_id, dtype=torch.long),\n",
    "            # \"genres_embed_ids\": padded_genres_embed_ids,\n",
    "            \"rating\": torch.tensor(rating, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import RatingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./data/processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RatingDataset(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_embed_id': tensor([ 1,  5,  7, 15, 17, 18, 19, 21, 27, 31]),\n",
       " 'movie_embed_id': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'rating': tensor([0.7778, 0.7778, 0.8889, 0.4444, 0.8889, 0.6667, 0.7778, 0.6667, 0.5556,\n",
       "         1.0000])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_embed_id': tensor([448, 119, 249, 534, 526,  88, 476, 279, 376,  31, 307, 144, 603, 346,\n",
       "         564, 387,   7, 313, 220, 312, 346, 561, 203, 337, 352, 182, 226, 599,\n",
       "         232, 410, 380, 419]),\n",
       " 'movie_embed_id': tensor([1125, 4707, 2874,  241, 2538,  754, 2141,   71, 1322,  505,  580,  204,\n",
       "         9360, 1609, 1993,   86,  447,  134,  198,  167, 6996,    5, 1343,  498,\n",
       "          273, 2188, 1270,  935, 2405, 2764, 6122, 3625]),\n",
       " 'rating': tensor([0.7778, 0.7778, 0.5556, 0.7778, 1.0000, 1.0000, 0.7778, 0.7778, 0.7778,\n",
       "         0.7778, 0.3333, 0.7778, 0.5556, 0.5556, 0.7778, 0.3333, 0.3333, 0.5556,\n",
       "         1.0000, 0.5556, 0.5556, 0.8889, 1.0000, 1.0000, 1.0000, 0.7778, 0.7778,\n",
       "         0.5556, 0.6667, 1.0000, 0.3333, 1.0000])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = NeuMF(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_embed_id': tensor([448, 119, 249, 534, 526,  88, 476, 279, 376,  31, 307, 144, 603, 346,\n",
       "         564, 387,   7, 313, 220, 312, 346, 561, 203, 337, 352, 182, 226, 599,\n",
       "         232, 410, 380, 419]),\n",
       " 'movie_embed_id': tensor([1125, 4707, 2874,  241, 2538,  754, 2141,   71, 1322,  505,  580,  204,\n",
       "         9360, 1609, 1993,   86,  447,  134,  198,  167, 6996,    5, 1343,  498,\n",
       "          273, 2188, 1270,  935, 2405, 2764, 6122, 3625]),\n",
       " 'rating': tensor([0.7778, 0.7778, 0.5556, 0.7778, 1.0000, 1.0000, 0.7778, 0.7778, 0.7778,\n",
       "         0.7778, 0.3333, 0.7778, 0.5556, 0.5556, 0.7778, 0.3333, 0.3333, 0.5556,\n",
       "         1.0000, 0.5556, 0.5556, 0.8889, 1.0000, 1.0000, 1.0000, 0.7778, 0.7778,\n",
       "         0.5556, 0.6667, 1.0000, 0.3333, 1.0000])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5117],\n",
       "        [0.5055],\n",
       "        [0.5059],\n",
       "        [0.5039],\n",
       "        [0.5158],\n",
       "        [0.4991],\n",
       "        [0.5021],\n",
       "        [0.5065],\n",
       "        [0.5117],\n",
       "        [0.5054],\n",
       "        [0.5023],\n",
       "        [0.5033],\n",
       "        [0.5062],\n",
       "        [0.5020],\n",
       "        [0.5057],\n",
       "        [0.4935],\n",
       "        [0.5007],\n",
       "        [0.4983],\n",
       "        [0.5031],\n",
       "        [0.5048],\n",
       "        [0.5032],\n",
       "        [0.5098],\n",
       "        [0.5105],\n",
       "        [0.5067],\n",
       "        [0.5006],\n",
       "        [0.5002],\n",
       "        [0.5077],\n",
       "        [0.5015],\n",
       "        [0.5110],\n",
       "        [0.4970],\n",
       "        [0.5012],\n",
       "        [0.5022]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender(user_indices=sample['user_embed_id'],\n",
    "            item_indices=sample['movie_embed_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
